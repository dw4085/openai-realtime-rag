{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f74fa173-6ff2-467e-af12-622a30b2231a",
   "metadata": {},
   "source": [
    "# Vector Database Collection & API Setup\n",
    "\n",
    "This notebook will walk you through setting up the vector database portion of the [openai-realtime-rag](https://github.com/ALucek/openai-realtime-rag/tree/main) fork."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270b5b9d-5aa2-4a07-8d2b-2de0c9c6bfb4",
   "metadata": {},
   "source": [
    "## Setting Up Your Vector Database\n",
    "\n",
    "For our vector database, a classic choice I use is [ChromaDB](https://www.trychroma.com/). While you can host Chroma as a server itself, I've decoupled the database and the API to allow for more dynamic plug and play capabilities for databases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa07129-78ad-4e64-8818-c2d51e3d6bda",
   "metadata": {},
   "source": [
    "#### Instantiate ChromaDB\n",
    "\n",
    "Create a persistent client of ChromaDB that will store everything in the folder `chroma`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d3c036b-0cff-4fa6-b42e-5619429df7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "# Creating Vector Database\n",
    "client = chromadb.PersistentClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6e599f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.delete_collection(name=\"vdb_collection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bd5c72-3905-4062-b561-2238b16512c1",
   "metadata": {},
   "source": [
    "#### Create a New Collection\n",
    "\n",
    "This is where all of our chunked text documents are going to be inserted into"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13d874dd-23e7-4766-99a9-899bca877a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = client.get_or_create_collection(name=\"vdb_collection\", metadata={\"hnsw:space\": \"cosine\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2ae864-a47a-4b01-a785-d015cb053b0a",
   "metadata": {},
   "source": [
    "#### Load & Split PDF \n",
    "\n",
    "We'll be using some simple LangChain integrations to load and chunk our PDF. Using OpenAI's standard token chunk size and overlap for their Assistants API as a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "430078d0-7ec1-4a78-9f9b-569e4a3b2c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Loading and Chunking\n",
    "loader = PyMuPDFLoader(\"./Rivian-Case-GPT.pdf\")\n",
    "pages = loader.load()\n",
    "\n",
    "#loader = PyPDFDirectoryLoader(\"./PDFs/\")\n",
    "#pages = loader.load()\n",
    "\n",
    "document = \"\"\n",
    "for i in range(len(pages)):\n",
    "    document += pages[i].page_content\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    model_name=\"gpt-4\",\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=400,\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_text(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73682f57-99fa-4e1d-8d87-96ac2aef779d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db47642d-ede7-42bb-9323-e9170ebe01c7",
   "metadata": {},
   "source": [
    "#### Insert Chunks into VDB Collection\n",
    "\n",
    "Embed each chunk into the collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce459ee9-69a9-4387-bb1b-d101916390f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert Chunks into ChromaDB Collection\n",
    "i = 0\n",
    "for chunk in chunks:\n",
    "    collection.add(\n",
    "    documents=[chunk],\n",
    "    ids=[f\"chunk_{i}\"]\n",
    "    )\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022acb9c-4f13-47a5-9453-003101da178a",
   "metadata": {},
   "source": [
    "---\n",
    "## API Setup\n",
    "\n",
    "We'll be using [FastAPI](https://fastapi.tiangolo.com/) as a quick and easy way to host our query function as a REST API. This API is what will be called from the defined `query_db` tool in the main console file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f958037-4293-4f09-9231-24da27a49956",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# Define a request model\n",
    "class QueryRequest(BaseModel):\n",
    "    query: str\n",
    "\n",
    "# Define the query endpoint\n",
    "@app.post(\"/query\")\n",
    "async def query_chroma(request: QueryRequest):\n",
    "    # Perform the query on your ChromaDB collection\n",
    "    results = collection.query(query_texts=[request.query], n_results=5)\n",
    "    return {\"results\": results['documents'][0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603b3cfb-8067-400a-8f77-eecceb93122c",
   "metadata": {},
   "source": [
    "#### Run API\n",
    "\n",
    "Using uvicorn to host the API as a local web server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "582c4a6f-77bc-447a-9e56-95c84033dd8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [20874]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "ERROR:    [Errno 48] error while attempting to bind on address ('0.0.0.0', 8000): address already in use\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:54029 - \"GET /docs HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:54029 - \"GET /openapi.json HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:54048 - \"OPTIONS /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:54048 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:54140 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:54175 - \"POST /query HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "import uvicorn\n",
    "import threading\n",
    "\n",
    "def run_api():\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "\n",
    "# Run the FastAPI app in a background thread\n",
    "thread = threading.Thread(target=run_api)\n",
    "thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a471babe-b2f8-4e3d-aeac-03e076a6e112",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
